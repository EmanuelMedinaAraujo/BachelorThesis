Server:
    @tars.cps.in.tum.de
    nvidia-smi
    Resourcen nicht überlegen

    Create tmux session:
     tmux new -s name
    acitvate conda environment:
        conda activate thesis_env
    To detach tmux:
        Ctrl+b, d
    tmux attach

Frage:
nohup vs screen vs tmux? Ist eins viel besser?

DONE:
- ssh keys erstellen
- conda einrichten
- Visualisierung:
    - Zusammenfassung verschiedener sample rausnehmen (error parameter rausnehmen)
    - joints kleiner machen / komplett ausblenden
    - endeffector in visualisierung erstellen

- prime-leaf21 lauf mit grafik laufen lassen (https://wandb.ai/medina-tum/thesis-pt-03/runs/iomgb1qp/overview)
    - distribution sollte kleiner werden (gamma ist dafür verantwortlich)
    https://wandb.ai/medina-tum/thesis-pt-03/runs/05i950rg/overview

- gamma auf 0 setzen
        https://wandb.ai/medina-tum/thesis-pt-03/runs/05i950rg/overview (s. Non-recurrent)
- recurrent vs non-recurrent vergleichen
    - Laufen lassen
        - Recurrent: https://wandb.ai/medina-tum/thesis-pt-03/runs/kp7m81h1/overview
        - Non-Recurrent: https://wandb.ai/medina-tum/thesis-pt-03/runs/05i950rg/overview
            - Non-Recurrent Base: (Gamma, Vis, TestInterval halbiert) https://wandb.ai/medina-tum/thesis-pt-03/runs/iomgb1qp/overview
                -> Gamma Wert war verantwortlich für die verteilung. mit 0. nicht mehr gestreut in visualisierung
- mit entropy coefficient rumspielen (0.01, (im paper))
    - Laufen lassen
        - https://wandb.ai/medina-tum/thesis-compare-ppo/runs/641h9cwa

In Progress:
- 5 Runs (1 Random seed - 42)
    - 1 Normal
        https://wandb.ai/medina-tum/thesis-compare-ppo/runs/alxzzve9
    - 1 mit erhöhter log_std_init (2.0 [default 0.0])
        https://wandb.ai/medina-tum/thesis-compare-ppo/runs/y749d0gk
    - 1 mit verkleinerter log_std_init (-3.0)
        https://wandb.ai/medina-tum/thesis-compare-ppo/runs/jdbdbsaf
    - 1 mit entropy coefficient 0.01 (sonst 0)
        https://wandb.ai/medina-tum/thesis-compare-ppo/runs/641h9cwa
    - 1 mit erhöhter log_std_init und entropy coefficient 0.01
        https://wandb.ai/medina-tum/thesis-compare-ppo/runs/o9t2yscc

    - Default Gamma (0.99; andere Runs haben 0.0)
        https://wandb.ai/medina-tum/thesis-compare-ppo/runs/2l7qvhi0

TODO:
- angles wie bei analytical berechnen
- seaborn anschauen und benutzen
    - pandas für data
- heatmap für endeffector erstellen
- ein zwei parameter sätze auswählen und value loss diskretisieren
    - heatmap für value loss erstellen (ein parameter pro grafik)




- Gymnasium für analytical erstellen
	wrapper
		base environment
			alle return sind torch.tensoren
		stable baselines environment erbt
			gibt numpy arrays zurück

- reread PPO optimizer Paper
- Read Adam optimizer Paper

--------------------------------------------------------------------------------------
Theoretical notes:
Neural Network Tradeoff:
Size vs. Difficulty of Training

ADAM Optimizer
    https://arxiv.org/abs/1412.6980

Nicht erreichbare Goals erst für Evaluierung Interessant

--------------------------------------------------------------------------------------
Latex Tipps:

Bibliography (settings.tex)
- NumerischerRreference style (wie beim IEEE) -  done
- konsistente Bibliography
	keep_tags:
	  - author
	  - editor (for books only)
	  - title
	  - chapter (for book chapters only)
	  - year
	  - booktitle (for books or conference articles)
	  - journal (for journal article only)
	  - number (for journal article only)
	  - volume (for journal article only)
	  - edition (for books only)
	  - pages
	  - howpublished (for misc [arXiv] only)
	Informationen müssen manuel zusammen gesucht werden
	Python Skript zur Überprüfung am Ende verwenden
		
- Sachen die man häufig braucht - Latex Commands anlegen
	https://github.com/JmlrOrg/tmlr-style-file/blob/main/math_commands.tex
- Formatting Template benutzen
- Custom Commands benutzen